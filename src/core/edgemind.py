"""
EdgeMind Core Integration v0.3.0
Revolutionary Local AI System with Multi-Model Support
"""

import os
import sys
import json
import time
import psutil
from pathlib import Path
from typing import Optional, Dict, Any, List, Union
from dataclasses import dataclass
from enum import Enum

# Fix imports for direct execution
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from agents.safe_computer_control import SafeComputerControl
    from core.smart_rag import SmartRAG
    from models.gpt_oss_integration import GPTOSSIntegration
    MODULES_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Some modules not available: {e}")
    MODULES_AVAILABLE = False
    # Create mock classes so EdgeMind can still run
    class SafeComputerControl:
        def is_safe(self, prompt): return True
    class SmartRAG:
        def get_context(self, prompt): return None
    class GPTOSSIntegration:
        def __call__(self, prompt, **kwargs):
            return {"choices": [{"text": f"GPT-OSS Mock: {prompt[:50]}..."}]}

# Try to import optional dependencies
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False


class ModelType(Enum):
    """Available model types with their characteristics"""
    TINYLLAMA = "tinyllama"  # 670MB, 34 tok/s
    PHI2 = "phi-2"  # 1.6GB, 25 tok/s
    MISTRAL = "mistral-7b"  # 4.1GB, 12 tok/s
    CODELLAMA = "codellama-7b"  # 4.1GB, 10 tok/s
    GPT_OSS = "gpt-oss"  # OpenAI open models
    TOGETHER = "together-api"  # Together API fallback


@dataclass
class ModelConfig:
    """Configuration for each model"""
    name: str
    size_mb: int
    tokens_per_sec: int
    context_length: int
    quantization: str
    path: Optional[str] = None
    url: Optional[str] = None


class EdgeMind:
    """
    Core EdgeMind Integration - Unifying all AI capabilities
    
    Features:
    - Multi-model routing with automatic fallback
    - Local-first with cloud backup
    - Intelligent memory management
    - Performance monitoring
    - Safety controls
    """
    
    # Model configurations
    MODEL_CONFIGS = {
        ModelType.TINYLLAMA: ModelConfig(
            name="TinyLlama-1.1B",
            size_mb=670,
            tokens_per_sec=34,
            context_length=2048,
            quantization="Q4_K_M",
            url="https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
        ),
        ModelType.PHI2: ModelConfig(
            name="Phi-2",
            size_mb=1600,
            tokens_per_sec=25,
            context_length=2048,
            quantization="Q4_K_M",
            url="https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf"
        ),
        ModelType.MISTRAL: ModelConfig(
            name="Mistral-7B",
            size_mb=4100,
            tokens_per_sec=12,
            context_length=8192,
            quantization="Q4_K_M",
            url="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
        ),
    }
    
    def __init__(
        self,
        model_type: ModelType = ModelType.TINYLLAMA,
        models_dir: str = "./models",
        enable_rag: bool = True,
        enable_safety: bool = True,
        enable_monitoring: bool = True,
        verbose: bool = True
    ):
        """
        Initialize EdgeMind with specified configuration
        
        Args:
            model_type: Which model to use
            models_dir: Directory for model storage
            enable_rag: Enable RAG system
            enable_safety: Enable safety controls
            enable_monitoring: Enable performance monitoring
            verbose: Print status messages
        """
        self.model_type = model_type
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(exist_ok=True)
        self.verbose = verbose
        
        # Components
        self.model = None
        self.rag_system = None
        self.safety_system = None
        self.gpt_oss = None
        
        # Conversation history for context
        self.conversation_history = []
        
        # Performance tracking
        self.metrics = {
            "total_tokens": 0,
            "total_time": 0,
            "avg_tokens_per_sec": 0,
            "memory_usage_mb": 0,
            "model_loads": 0
        }
        
        # Initialize monitoring first if enabled
        if enable_monitoring:
            self._start_monitoring()
        
        # Initialize components
        self._log("üß† Initializing EdgeMind v0.3.0...")
        
        if enable_rag and MODULES_AVAILABLE:
            self._initialize_rag()
        
        if enable_safety and MODULES_AVAILABLE:
            self._initialize_safety()
        
        # Load model
        self.load_model(model_type)
        
        self._log("‚úÖ EdgeMind ready! Running 100% locally.")
    
    def _log(self, message: str):
        """Log message if verbose"""
        if self.verbose:
            print(f"[EdgeMind] {message}")
    
    def _initialize_rag(self):
        """Initialize RAG system"""
        try:
            self.rag_system = SmartRAG()
            self._log("üìö RAG system initialized")
        except Exception as e:
            self._log(f"‚ö†Ô∏è RAG initialization failed: {e}")
    
    def _initialize_safety(self):
        """Initialize safety controls"""
        try:
            self.safety_system = SafeComputerControl()
            self._log("üîí Safety controls activated")
        except Exception as e:
            self._log(f"‚ö†Ô∏è Safety initialization failed: {e}")
    
    def _start_monitoring(self):
        """Start performance monitoring"""
        self.start_time = time.time()
        self.initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
        self._log("üìä Performance monitoring enabled")
    
    def load_model(self, model_type: ModelType):
        """
        Load specified model with automatic download if needed
        
        Args:
            model_type: Type of model to load
        """
        config = self.MODEL_CONFIGS.get(model_type)
        if not config:
            raise ValueError(f"Unknown model type: {model_type}")
        
        model_path = self.models_dir / f"{config.name}.gguf"
        
        # Download if needed
        if not model_path.exists() and config.url:
            self._download_model(config.url, model_path)
        
        # Load model based on availability
        if LLAMA_CPP_AVAILABLE and model_path.exists():
            self._load_llama_cpp(model_path, config)
        elif model_type == ModelType.GPT_OSS:
            self._load_gpt_oss()
        elif model_type == ModelType.TOGETHER:
            self._load_together_api()
        else:
            self._log("‚ö†Ô∏è No suitable backend found, using mock mode")
            self.model = self._create_mock_model()
        
        self.metrics["model_loads"] += 1
    
    def _download_model(self, url: str, path: Path):
        """Download model from URL"""
        try:
            import requests
            from tqdm import tqdm
            
            self._log(f"üì• Downloading model to {path}...")
            
            response = requests.get(url, stream=True)
            total_size = int(response.headers.get('content-length', 0))
            
            with open(path, 'wb') as file:
                with tqdm(total=total_size, unit='iB', unit_scale=True) as pbar:
                    for data in response.iter_content(chunk_size=8192):
                        pbar.update(len(data))
                        file.write(data)
            
            self._log(f"‚úÖ Model downloaded: {path}")
        except Exception as e:
            self._log(f"‚ùå Download failed: {e}")
            self._log("‚ö†Ô∏è Continuing with mock model")
    
    def _load_llama_cpp(self, model_path: Path, config: ModelConfig):
        """Load model using llama-cpp-python"""
        self._log(f"üîß Loading {config.name} with llama.cpp...")
        
        try:
            self.model = Llama(
                model_path=str(model_path),
                n_ctx=config.context_length,
                n_batch=512,
                n_threads=psutil.cpu_count(logical=False),
                verbose=False,
                # Add stop tokens for TinyLlama
                stop=["<|user|>", "<|assistant|>", "<|system|>", "</s>"]
            )
            self._log(f"‚úÖ Model loaded: {config.tokens_per_sec} tokens/sec expected")
        except Exception as e:
            self._log(f"‚ùå Failed to load model: {e}")
            self.model = self._create_mock_model()
    
    def _load_gpt_oss(self):
        """Load GPT-OSS integration"""
        try:
            self.gpt_oss = GPTOSSIntegration()
            self.model = self.gpt_oss
            self._log("‚úÖ GPT-OSS models ready")
        except Exception as e:
            self._log(f"‚ö†Ô∏è GPT-OSS not available: {e}")
            self.model = self._create_mock_model()
    
    def _load_together_api(self):
        """Load Together API as fallback"""
        # This would integrate with your existing Together setup
        self._log("üåê Using Together API fallback")
        self.model = self._create_mock_model()  # For now, use mock
    
    def _create_mock_model(self):
        """Create mock model for testing"""
        class MockModel:
            def __call__(self, prompt, **kwargs):
                # More interesting mock responses
                responses = {
                    "intelligence": "Intelligence at the edge means processing data where it's created, bringing AI directly to you.",
                    "haiku": "Local AI runs fast,\nNo cloud delays or data leaks,\nYour device, your mind.",
                    "edge": "Edge computing brings processing power closer to data sources, reducing latency and improving privacy.",
                    "default": "This is a mock response. Install llama-cpp-python and download a model for real inference."
                }
                
                # Try to match keywords for better mock responses
                prompt_lower = prompt.lower()
                for key, response in responses.items():
                    if key in prompt_lower:
                        return {"choices": [{"text": response}]}
                
                return {"choices": [{"text": responses["default"]}]}
        
        return MockModel()
    
    def format_prompt_for_tinyllama(self, prompt: str) -> str:
        """
        Format prompt properly for TinyLlama chat model.
        TinyLlama expects a specific format for coherent responses.
        """
        return f"<|system|>\nYou are a helpful, respectful and honest assistant.\n<|user|>\n{prompt}\n<|assistant|>\n"
    
    def format_prompt(self, prompt: str, model_type: Optional[ModelType] = None) -> str:
        """
        Format prompt based on model type for better responses.
        Different models expect different prompt formats.
        """
        model_type = model_type or self.model_type
        
        if model_type == ModelType.TINYLLAMA:
            return self.format_prompt_for_tinyllama(prompt)
        elif model_type == ModelType.MISTRAL:
            # Mistral format
            return f"<s>[INST] {prompt} [/INST]"
        elif model_type == ModelType.PHI2:
            # Phi-2 format
            return f"Instruct: {prompt}\nOutput:"
        elif model_type == ModelType.CODELLAMA:
            # CodeLlama format
            return f"[INST] {prompt} [/INST]"
        else:
            # Default format
            return prompt
    
    def generate(
        self,
        prompt: str,
        max_tokens: int = 256,
        temperature: float = 0.7,
        stream: bool = False,
        use_rag: bool = False,
        safety_check: bool = True
    ) -> Union[str, Dict[str, Any]]:
        """
        Generate response with EdgeMind
        
        Args:
            prompt: Input prompt
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            stream: Stream response
            use_rag: Use RAG for context
            safety_check: Apply safety controls
            
        Returns:
            Generated text or full response dict
        """
        start_time = time.time()
        
        # Basic safety check for dangerous queries
        dangerous_keywords = [
            "bomb", "explosive", "weapon", "kill", "murder", "suicide",
            "terrorism", "make meth", "synthesize drugs", "poison"
        ]
        
        if any(keyword in prompt.lower() for keyword in dangerous_keywords):
            self._log("‚ö†Ô∏è Potentially dangerous query detected")
            return "I cannot and will not provide information about creating weapons, explosives, or other dangerous items. If you're interested in chemistry or engineering, I'd be happy to discuss safe and legal topics instead."
        
        # Adjust parameters for TinyLlama but respect user's temperature if explicitly set
        if self.model_type == ModelType.TINYLLAMA:
            max_tokens = min(max_tokens, 100)  # Limit TinyLlama responses
            # Only override temperature if it's the default 0.7
            if temperature == 0.7:
                temperature = 0.3  # Better default for TinyLlama
                self._log(f"üéØ Adjusted params: max_tokens={max_tokens}, temp={temperature}")
            else:
                self._log(f"üéØ Using user settings: max_tokens={max_tokens}, temp={temperature}")
        
        # RAG enhancement
        if use_rag and self.rag_system:
            try:
                context = self.rag_system.get_context(prompt)
                if context:
                    prompt = f"Context: {context}\n\nQuery: {prompt}"
                    self._log("üìö RAG context added")
            except Exception as e:
                self._log(f"‚ö†Ô∏è RAG error: {e}")
        
        # Safety check with existing system if available
        if safety_check and self.safety_system:
            try:
                if not self.safety_system.is_safe(prompt):
                    return "‚ö†Ô∏è Request blocked by safety system"
            except Exception as e:
                self._log(f"‚ö†Ô∏è Safety check error: {e}")
        
        # Format prompt for specific model if needed
        formatted_prompt = self.format_prompt(prompt)
        if formatted_prompt != prompt:
            self._log(f"üìù Applied {self.model_type.value} prompt formatting")
        
        # Generate response
        try:
            if isinstance(self.model, Llama):
                # Define stop tokens based on model
                stop_tokens = self._get_stop_tokens()
                
                # Add repetition penalty for TinyLlama
                repeat_penalty = 1.1 if self.model_type == ModelType.TINYLLAMA else 1.0
                
                response = self.model(
                    formatted_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    stream=stream,
                    stop=stop_tokens,
                    echo=False,  # Don't echo the prompt
                    repeat_penalty=repeat_penalty,  # Reduce repetition
                    top_p=0.95,  # Nucleus sampling
                    top_k=40  # Limit vocabulary
                )
                text = response["choices"][0]["text"]
                
                # Clean up response
                text = self._clean_response(text)
                
                # Check for repetitive patterns
                words = text.split()
                if len(words) > 10:
                    # Check if phrase is repeating
                    for i in range(3, min(10, len(words)//2)):
                        chunk = " ".join(words[:i])
                        if text.count(chunk) > 2:
                            # Cut off at first repetition
                            text = chunk
                            self._log("‚úÇÔ∏è Trimmed repetitive output")
                            break
                
            else:
                # Fallback to mock or API
                response = self.model(formatted_prompt, max_tokens=max_tokens)
                text = response["choices"][0]["text"]
        except Exception as e:
            self._log(f"‚ùå Generation error: {e}")
            text = "Error generating response. Please check model configuration."
        
        # Update metrics
        elapsed = time.time() - start_time
        tokens = len(text.split())
        self.metrics["total_tokens"] += tokens
        self.metrics["total_time"] += elapsed
        self.metrics["avg_tokens_per_sec"] = self.metrics["total_tokens"] / max(self.metrics["total_time"], 1)
        
        if self.verbose:
            self._log(f"‚ö° Generated {tokens} tokens in {elapsed:.2f}s ({tokens/elapsed:.1f} tok/s)")
        
        return text if not stream else response
    
    def _get_stop_tokens(self) -> List[str]:
        """Get stop tokens based on model type"""
        if self.model_type == ModelType.TINYLLAMA:
            return ["<|user|>", "<|assistant|>", "<|system|>", "</s>", "\n<|"]
        elif self.model_type == ModelType.MISTRAL:
            return ["</s>", "[INST]", "[/INST]"]
        elif self.model_type == ModelType.PHI2:
            return ["Instruct:", "Output:", "\n\n"]
        else:
            return ["</s>", "\n\n\n"]
    
    def _clean_response(self, text: str) -> str:
        """Clean up model response by removing template tokens"""
        # Remove common template tokens
        cleanup_tokens = [
            "<|user|>", "<|assistant|>", "<|system|>",
            "</s>", "[INST]", "[/INST]",
            "Instruct:", "Output:"
        ]
        
        for token in cleanup_tokens:
            text = text.replace(token, "")
        
        # Remove excessive whitespace
        text = " ".join(text.split())
        
        return text.strip()
    
    def benchmark(self, prompts: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Run benchmark on model
        
        Args:
            prompts: List of prompts to test
            
        Returns:
            Benchmark results
        """
        if not prompts:
            prompts = [
                "What is the capital of France?",
                "Explain quantum computing in simple terms.",
                "Write a Python function to sort a list.",
                "What are the benefits of exercise?",
                "How does photosynthesis work?"
            ]
        
        results = []
        self._log("üèÉ Running benchmark...")
        
        for prompt in prompts:
            start = time.time()
            response = self.generate(prompt, max_tokens=100, safety_check=False)
            elapsed = time.time() - start
            
            results.append({
                "prompt": prompt[:50] + "...",
                "response_length": len(response),
                "time": elapsed,
                "tokens_per_sec": len(response.split()) / elapsed if elapsed > 0 else 0
            })
        
        # Calculate averages
        avg_time = sum(r["time"] for r in results) / len(results)
        avg_tokens = sum(r["tokens_per_sec"] for r in results) / len(results)
        
        # Memory usage
        current_memory = psutil.Process().memory_info().rss / 1024 / 1024
        memory_used = current_memory - self.initial_memory if hasattr(self, 'initial_memory') else 0
        
        benchmark_results = {
            "model": self.model_type.value,
            "prompts_tested": len(prompts),
            "avg_response_time": avg_time,
            "avg_tokens_per_sec": avg_tokens,
            "memory_usage_mb": memory_used,
            "detailed_results": results
        }
        
        self._log(f"""
‚úÖ Benchmark Complete!
üìä Model: {self.model_type.value}
‚ö° Speed: {avg_tokens:.1f} tokens/sec
üíæ Memory: {memory_used:.1f} MB
‚è±Ô∏è Avg Response: {avg_time:.2f}s
        """)
        
        return benchmark_results
    
    def chat(self):
        """Interactive chat mode"""
        self._log("""
üí¨ Chat Mode - EdgeMind v0.3.0
Commands:
  /rag <query>  - Search with RAG
  /safe <query> - Check safety
  /bench       - Run benchmark
  /switch <model> - Switch model
  /temp <value> - Set temperature (0.1-1.0)
  /clear       - Clear conversation history
  /quit        - Exit
        """)
        
        if self.model_type == ModelType.TINYLLAMA:
            self._log("üìù Using TinyLlama with proper prompt formatting")
            self._log("üí° Tip: Lower temperature (0.3) for better responses")
        
        chat_temperature = 0.3  # Default for chat mode
        
        while True:
            try:
                user_input = input("\nüë§ You: ").strip()
                
                if user_input.lower() == '/quit':
                    break
                elif user_input.lower() == '/clear':
                    self.conversation_history = []
                    self._log("üóëÔ∏è Conversation history cleared")
                elif user_input.lower() == '/bench':
                    self.benchmark()
                elif user_input.startswith('/temp'):
                    try:
                        new_temp = float(user_input.split()[1])
                        if 0.1 <= new_temp <= 1.0:
                            chat_temperature = new_temp
                            self._log(f"üå°Ô∏è Temperature set to {chat_temperature}")
                        else:
                            self._log("‚ùå Temperature must be between 0.1 and 1.0")
                    except (IndexError, ValueError):
                        self._log("‚ùå Usage: /temp 0.3")
                elif user_input.startswith('/switch'):
                    model_name = user_input.split(' ', 1)[1] if ' ' in user_input else 'tinyllama'
                    try:
                        self.load_model(ModelType(model_name))
                        self.conversation_history = []  # Clear history on model switch
                    except ValueError:
                        self._log(f"‚ùå Unknown model: {model_name}")
                elif user_input.startswith('/rag'):
                    query = user_input.split(' ', 1)[1] if ' ' in user_input else ''
                    response = self.generate(query, use_rag=True, temperature=chat_temperature)
                    print(f"\nü§ñ EdgeMind: {response}")
                elif user_input.startswith('/safe'):
                    query = user_input.split(' ', 1)[1] if ' ' in user_input else ''
                    is_safe = self.safety_system.is_safe(query) if self.safety_system else True
                    print(f"\nüîí Safety Check: {'‚úÖ SAFE' if is_safe else '‚ö†Ô∏è UNSAFE'}")
                else:
                    response = self.generate(user_input, temperature=chat_temperature, max_tokens=100)
                    print(f"\nü§ñ EdgeMind: {response}")
                    
                    # Add to conversation history
                    self.conversation_history.append({"role": "user", "content": user_input})
                    self.conversation_history.append({"role": "assistant", "content": response})
                    
                    # Keep only last 10 exchanges
                    if len(self.conversation_history) > 20:
                        self.conversation_history = self.conversation_history[-20:]
                    
            except KeyboardInterrupt:
                break
            except Exception as e:
                self._log(f"‚ùå Error: {e}")
        
        self._log("üëã Goodbye!")
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get current performance metrics"""
        if hasattr(self, 'initial_memory'):
            current_memory = psutil.Process().memory_info().rss / 1024 / 1024
            self.metrics["memory_usage_mb"] = current_memory - self.initial_memory
        return self.metrics


def main():
    """Main entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(description="EdgeMind - Local AI System")
    parser.add_argument("--model", default="tinyllama", help="Model to use")
    parser.add_argument("--benchmark", action="store_true", help="Run benchmark")
    parser.add_argument("--chat", action="store_true", help="Start chat mode")
    parser.add_argument("--prompt", type=str, help="Single prompt to run")
    
    args = parser.parse_args()
    
    # Initialize EdgeMind
    print("\n" + "="*60)
    print("üß† EdgeMind v0.3.0 - Local AI System")
    print("üìù Now with TinyLlama prompt formatting fix!")
    print("="*60)
    
    try:
        edgemind = EdgeMind(
            model_type=ModelType(args.model),
            enable_rag=MODULES_AVAILABLE,
            enable_safety=MODULES_AVAILABLE,
            enable_monitoring=True
        )
    except ValueError:
        print(f"‚ùå Unknown model: {args.model}")
        print(f"Available models: {', '.join([m.value for m in ModelType])}")
        return
    
    if args.benchmark:
        edgemind.benchmark()
    elif args.chat:
        edgemind.chat()
    elif args.prompt:
        response = edgemind.generate(args.prompt)
        print(f"\nüìù Prompt: {args.prompt}")
        print(f"ü§ñ Response: {response}")
    else:
        # Demo mode
        print("\nüéØ EdgeMind Demo - Running locally with zero cloud dependency!")
        print("-" * 60)
        
        demo_prompts = [
            "What is artificial intelligence?",
            "Write a haiku about local AI",
            "Explain the benefits of edge computing"
        ]
        
        for prompt in demo_prompts:
            print(f"\nüìù Prompt: {prompt}")
            response = edgemind.generate(prompt, max_tokens=100)
            print(f"ü§ñ Response: {response}")
        
        print("\n" + "-" * 60)
        print("üìä Session Metrics:")
        metrics = edgemind.get_metrics()
        for key, value in metrics.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.2f}")
            else:
                print(f"  {key}: {value}")
        
        print("\nüí° Tips:")
        print("  - Run with --chat for interactive mode")
        print("  - Run with --benchmark to test performance")
        print("  - Run with --model mistral-7b for larger model")
        print("\n‚ú® EdgeMind: Your AI, Your Device, Your Control")


if __name__ == "__main__":
    main()