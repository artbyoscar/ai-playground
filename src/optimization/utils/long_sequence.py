import torch
import gc
import logging
import numpy as np
import time
from tqdm import tqdm

logger = logging.getLogger('edgeformer')

def process_long_document(model, document, chunk_size=4096, overlap=512, show_progress=True):
    """Process a document by breaking it into overlapping chunks.
    
    Args:
        model: The EdgeFormer model to use for processing
        document: Input tokens or text to process
        chunk_size: Maximum sequence length to process at once
        overlap: Number of tokens to overlap between chunks
        show_progress: Whether to show a progress bar
        
    Returns:
        Merged processing results
    """
    chunks = []
    
    # Handle the case where document is already tokenized (as tensor)
    if isinstance(document, torch.Tensor):
        # Calculate number of chunks needed
        effective_chunk_size = chunk_size - overlap
        seq_length = document.size(1)
        num_chunks = max(1, int(np.ceil((seq_length - overlap) / effective_chunk_size)))
        
        logger.info(f"Processing sequence of length {seq_length} in {num_chunks} chunks "
                    f"(chunk_size={chunk_size}, overlap={overlap})")
        
        # Process each chunk
        chunk_iterator = range(0, seq_length, chunk_size - overlap)
        if show_progress:
            chunk_iterator = tqdm(chunk_iterator, desc="Processing chunks", total=num_chunks)
        
        for i in chunk_iterator:
            # Extract chunk with attention to boundaries
            end_idx = min(i + chunk_size, document.size(1))
            chunk = document[:, i:end_idx]
            
            # Process each chunk independently
            chunk_start_time = time.time()
            with torch.no_grad():
                result = model(chunk)
            chunk_end_time = time.time()
            
            # Log chunk processing time
            logger.debug(f"Chunk {len(chunks)+1}/{num_chunks} processed in {chunk_end_time - chunk_start_time:.2f}s")
            
            chunks.append(result)
            
            # Apply strategic garbage collection
            if len(chunks) > 1 and len(chunks) % 2 == 0:
                logger.debug("Applying strategic garbage collection")
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
    
    # Handle the case where document is raw text (needs tokenization)
    else:
        # This assumes your model has a tokenizer or some way to create input_ids
        # You'll need to adapt this based on your actual tokenization approach
        tokenized_chunks = []
        
        # Calculate number of chunks
        effective_chunk_size = chunk_size - overlap
        num_chunks = max(1, int(np.ceil((len(document) - overlap) / effective_chunk_size)))
        
        logger.info(f"Processing text of length {len(document)} in {num_chunks} chunks "
                   f"(chunk_size={chunk_size}, overlap={overlap})")
        
        # Split into chunks
        chunk_iterator = range(0, len(document), chunk_size - overlap)
        if show_progress:
            chunk_iterator = tqdm(chunk_iterator, desc="Processing text chunks", total=num_chunks)
        
        for i in chunk_iterator:
            # Extract chunk with attention to boundaries
            end_idx = min(i + chunk_size, len(document))
            text_chunk = document[i:end_idx]
            
            # Convert to input_ids (adapt based on your tokenization approach)
            input_ids = torch.tensor([[ord(c) % model.config.vocab_size for c in text_chunk]])
            
            tokenized_chunks.append(input_ids)
        
        # Process each tokenized chunk
        for idx, input_ids in enumerate(tokenized_chunks):
            chunk_start_time = time.time()
            with torch.no_grad():
                result = model(input_ids)
            chunk_end_time = time.time()
            
            # Log chunk processing time
            logger.debug(f"Chunk {idx+1}/{len(tokenized_chunks)} processed in {chunk_end_time - chunk_start_time:.2f}s")
            
            chunks.append(result)
            
            # Apply strategic garbage collection
            if idx > 0 and idx % 2 == 0:
                logger.debug("Applying strategic garbage collection")
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
    
    # Merge results, handling the overlap
    if len(chunks) == 1:
        return chunks[0]
    
    # For logits or embeddings, we need special handling
    if "logits" in chunks[0]:
        # Initialize with the first chunk
        final_result = chunks[0].copy() if hasattr(chunks[0], 'copy') else {k: v for k, v in chunks[0].items()}
        
        # Merge remaining chunks, discarding overlap at the beginning
        for i in range(1, len(chunks)):
            # For logits, we need to concatenate along the sequence length dimension (dim=1)
            overlap_size = overlap
            final_result["logits"] = torch.cat(
                [final_result["logits"], chunks[i]["logits"][:, overlap_size:]], 
                dim=1
            )
            
            # Handle other outputs like hidden states if needed
            if "hidden_states" in final_result and final_result["hidden_states"] is not None:
                final_hidden_states = []
                for j, hidden_state in enumerate(final_result["hidden_states"]):
                    final_hidden_states.append(
                        torch.cat([hidden_state, chunks[i]["hidden_states"][j][:, overlap_size:]], dim=1)
                    )
                final_result["hidden_states"] = tuple(final_hidden_states)
    else:
        # For simpler outputs or if specific handling is needed
        # You may need to adapt this based on your model's output format
        final_result = chunks[0]
        for i in range(1, len(chunks)):
            if hasattr(final_result, "__add__"):
                # If result supports addition
                final_result += chunks[i][overlap:]
            else:
                # Fallback for dictionary-like results
                logger.warning("Unsupported result format for merging. Returning first chunk only.")
                return chunks[0]
    
    return final_result

def memory_aware_forward(model, input_ids, attention_type="auto", max_memory_mb=None):
    """
    Forward pass with automatic memory management strategies.
    
    Args:
        model: EdgeFormer model
        input_ids: Input token ids
        attention_type: "auto", "standard", "mla", or "mla_window"
        max_memory_mb: Maximum memory to use in MB, or None to auto-detect
        
    Returns:
        Model output
    """
    seq_length = input_ids.shape[1]
    
    # Auto-detect maximum memory if not specified
    if max_memory_mb is None:
        if torch.cuda.is_available():
            max_memory_mb = torch.cuda.get_device_properties(0).total_memory / (1024 * 1024)
            # Use 80% of available memory to leave some for overhead
            max_memory_mb *= 0.8
        else:
            # On CPU, estimate based on system memory (rough estimation)
            import psutil
            max_memory_mb = psutil.virtual_memory().available / (1024 * 1024) * 0.5
    
    logger.info(f"Memory-aware forward pass with sequence length {seq_length}, "
                f"max memory {max_memory_mb:.2f} MB")
    
    # Estimate memory requirements based on sequence length
    # These formulas are approximations - you should calibrate them for your model
    estimated_mem_standard = 0.01 * seq_length * seq_length + 10  # Quadratic relationship with seq length
    estimated_mem_mla = 0.005 * seq_length * seq_length + 15      # MLA has lower memory requirements
    estimated_mem_mla_window = 0.003 * seq_length * seq_length + 20  # Window attention has lowest memory requirements
    
    logger.debug(f"Estimated memory requirements - "
                f"Standard: {estimated_mem_standard:.2f} MB, "
                f"MLA: {estimated_mem_mla:.2f} MB, "
                f"MLA Window: {estimated_mem_mla_window:.2f} MB")
    
    # Determine which attention mechanism to use
    if attention_type == "auto":
        if estimated_mem_standard < max_memory_mb and seq_length < 4096:
            # For shorter sequences, standard attention is faster and fits in memory
            selected_attention = "standard"
        elif estimated_mem_mla < max_memory_mb and seq_length < 8192:
            # For medium sequences, MLA is a good balance
            selected_attention = "mla"
        elif estimated_mem_mla_window < max_memory_mb and seq_length < 16384:
            # For longer sequences, use MLA with sliding window
            selected_attention = "mla_window"
        else:
            # For very long sequences, use chunking
            logger.info(f"Sequence length {seq_length} exceeds memory capacity, using chunking")
            return process_long_document(model, input_ids)
    else:
        selected_attention = attention_type
    
    logger.info(f"Selected attention mechanism: {selected_attention}")
    
    # Set the attention type before forward pass
    # Assuming the model has a set_attention_type method or attention_type attribute
    if hasattr(model, 'set_attention_type'):
        model.set_attention_type(selected_attention)
    elif hasattr(model, 'attention_type'):
        model.attention_type = selected_attention
    
    # Strategic garbage collection before forward pass
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # Perform the forward pass
    with torch.no_grad():
        outputs = model(input_ids)
    
    return outputs

def quantize_model(model, quantization_type="int8"):
    """Apply quantization to model weights.
    
    Args:
        model: The EdgeFormer model to quantize
        quantization_type: Type of quantization to apply ('int8', 'int4', or 'fp16')
        
    Returns:
        Quantized version of the model
    """
    logger = logging.getLogger('edgeformer')
    logger.info(f"Applying {quantization_type} quantization to model")
    
    if quantization_type == "int8":
        try:
            # INT8 quantization
            import torch.quantization
            
            # Create a copy of the model
            quantized_model = type(model)(model.config)
            quantized_model.load_state_dict(model.state_dict())
            
            # Specify quantization configuration
            quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
            
            # Prepare for quantization (fuse operations like Conv+BN+ReLU)
            if hasattr(torch.quantization, 'prepare'):
                logger.info("Preparing model for quantization...")
                torch.quantization.prepare(quantized_model, inplace=True)
                
                # Calibration would happen here with representative data
                
                # Convert to quantized model
                logger.info("Converting to quantized model...")
                torch.quantization.convert(quantized_model, inplace=True)
            else:
                logger.warning("torch.quantization.prepare not available, using simulated quantization")
                # Simulate quantization manually
                state_dict = quantized_model.state_dict()
                for name, param in state_dict.items():
                    if param.dtype == torch.float32:
                        # Skip non-tensor parameters and embeddings
                        if not isinstance(param, torch.Tensor) or "embed" in name:
                            continue
                        
                        # Simulate int8 quantization
                        with torch.no_grad():
                            # Calculate scale
                            abs_max = torch.max(torch.abs(param))
                            scale = 127.0 / abs_max
                            
                            # Quantize
                            quantized = torch.round(param * scale)
                            quantized = torch.clamp(quantized, -128, 127)
                            
                            # Dequantize
                            dequantized = quantized / scale
                            
                            # Store the dequantized values
                            state_dict[name] = dequantized
                
                # Load the simulated quantized state dict
                quantized_model.load_state_dict(state_dict)
            
            return quantized_model
            
        except Exception as e:
            logger.error(f"Error during INT8 quantization: {e}")
            logger.info("Falling back to simulated quantization")
            # Fallback to simulated quantization
            return simulate_quantization(model, bits=8)
        
    elif quantization_type == "int4":
        logger.info("Using simulated INT4 quantization")
        return simulate_quantization(model, bits=4)
        
    elif quantization_type == "fp16":
        # FP16 quantization (half precision)
        logger.info("Converting model to FP16 precision")
        model_fp16 = model.half()
        return model_fp16
    else:
        logger.warning(f"Unknown quantization type: {quantization_type}. Returning original model.")
        return model

def simulate_quantization(model, bits=8):
    """
    Simulate quantization by quantizing and dequantizing weights.
    
    Args:
        model: Model to quantize
        bits: Number of bits (4 or 8)
        
    Returns:
        Model with simulated quantized weights
    """
    logger = logging.getLogger('edgeformer')
    logger.info(f"Simulating {bits}-bit quantization")
    
    # Create a copy of the model
    quantized_model = type(model)(model.config)
    quantized_model.load_state_dict(model.state_dict())
    
    # Calculate max value based on bits
    max_val = 2**(bits-1) - 1
    
    # Get state dict
    state_dict = quantized_model.state_dict()
    
    # Quantize each parameter
    for name, param in state_dict.items():
        # Skip non-tensor parameters
        if not isinstance(param, torch.Tensor):
            continue
            
        # Skip parameters that shouldn't be quantized (e.g., embeddings)
        if "embed" in name:
            continue
            
        # Perform simulated quantization
        with torch.no_grad():
            # Calculate scale
            abs_max = torch.max(torch.abs(param))
            scale = max_val / abs_max
            
            # Quantize
            quantized = torch.round(param * scale)
            quantized = torch.clamp(quantized, -max_val, max_val)
            
            # Dequantize
            dequantized = quantized / scale
            
            # Store the dequantized values
            state_dict[name] = dequantized
    
    # Load the quantized state dict
    quantized_model.load_state_dict(state_dict)
    
    return quantized_model